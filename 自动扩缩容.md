[TOC]

### K8S 实现自动扩缩容

---

#### pod的横向自动伸缩

横向pod自动伸缩是指由控制器管理的pod副本数量的自动伸缩。 它由 Horizontal控制器执行， 我们通过创建 一 个HorizontalpodAutoscaler （HPA)资源来启用和配置Horizontal控制器。 该控制器周期性检查pod度量， 计算满足HPA 资源所配置的目标 数 值所 需 的 副本数量， 进而调整目标资源(如Deployment、 ReplicaSet、 ReplicationController、 StatefulSet等)的replicas字段。

##### 了解伸缩过程

自动伸缩 的过程可以分为三个步骤:

- 获取被伸缩资源对象所管理的所有pod度量。
- 计算使度量数值到达(或接近)所指定目标数值所需的pod数量。
- 更新被伸缩资源的replicas字段。

##### 获取度量

![img](https://static001.geekbang.org/resource/image/8f/9e/8f4a22788c03b06377cabe791c67989e.png?wh=1562x572)

如图所示， Autoscaler本身并不负责采集pod度量数据，而是cAdvisor收集容器数据，交与Metrics-Server进行聚合，hpa直接从MetricsServer获取数据计算度量并更改replicas数量

##### 计算度量

![截屏2022-09-12 23.37.17](/Users/sumyf/Desktop/截屏2022-09-12 23.37.17.png)

- 计算单个度量，会将多个副本相加除以目标值向上取整得到目标数
- 当计算多个度量的时候，则按单个度量计算后取最大值

##### 更新被伸缩的资源的副本数

Autoscaler控制器通过Scale子资源来修改被伸缩资源的replicas字段。这样Autoscaler不必了解它所管理资源的细节，而只需要通过Scale子资源暴露的界面，就可以完成它的工作了。
![截屏2022-09-12 23.48.08](/Users/sumyf/Documents/code/docker/doc/image/截屏2022-09-12 23.48.08.png)

支持修改的资源有： 

- Deployment
- ReplicaSet
- ReplicationController
- StatefulSet

#### 基于CPU使用率进行伸缩

创建要被伸缩的deployment

```yaml
# deployment.yaml 
 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  selector:
    matchLabels:
      app: kubia
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
        resources:
          requests:
            cpu: 100m
```

加上自动伸缩

```
kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5
```

```yaml
kubectl get hpa -o yaml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: "2022-09-12T15:56:57Z"
  name: kubia
  namespace: default
  resourceVersion: "117954"
  uid: 1f0b5e7f-e4a7-4d47-be5c-57a3da09223a
spec:
  maxReplicas: 5   # 伸缩最大数量
  metrics:  # 声明计算的指标
  - resource:
      name: cpu
      target:
        averageUtilization: 30 # 目标值是30
        type: Utilization
    type: Resource
  minReplicas: 1 #伸缩最小值
  scaleTargetRef: # 关联的资源，也就是被操作伸缩的资源
    apiVersion: apps/v1
    kind: Deployment
    name: kubia
```

模拟请求，并观察伸缩情况

```sh
# 暴露deployment的服务
kubectl expose deploy kubia --port=80 --target-port=8080

# 使用ab 压测
kubectl run test -it --image=httpd:alpine -- sh

ab -c 10 -t 60 -n 1000000 'http://kubia/'
```

#### 基于内存进行伸缩

基于内存的自动伸缩比基于CPU的困难很多。 主要原因在于，扩容之后原有的 pod 需要有办法释放内存 。 这只能由应用完成，系统无法代劳。 系统所能做的只有杀死并重启应用，希望它能比之前少占用 一些内存；但如果应用使用了跟之前一样多的内存 ， Autoscaler 就会扩容、扩容 ， 再扩容 ， 直到达到 HPA 资源上配置的最大 pod 数量 。 显然没有 人想 要这种行为 。

#### 基于自定义度量进行伸缩

在这个演示中，我们将部署一个应用程序 mockmetrics，它将在`/metrics`. 这些指标将被 Prometheus 抓取。在 的帮助下[`k8s-prometheus-adapter`](https://github.com/DirectXMan12/k8s-prometheus-adapter)，我们将创建 APIService `custom.metrics.k8s.io`，然后 HPA 将使用它来扩展 mockmetrics 应用程序的部署（增加副本数量）。

##### 安装helm

##### 安装 Prometheus Operator 和 Prometheus

现在，我们将安装[kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) chart。它将部署[Prometheus Operator](https://github.com/coreos/prometheus-operator)并使用它创建 Prometheus 实例。

- 添加 prometheus-community Helm 存储库并创建`monitoring`命名空间。

  ```
  $ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  "prometheus-community" has been added to your repositories
  
  $ helm repo add stable https://mirror.azure.cn/kubernetes/charts/
  "stable" has been added to your repositories
  
  $ helm repo update
  Hang tight while we grab the latest from your chart repositories...
  ...Successfully got an update from the "prometheus-community" chart repository
  ...Successfully got an update from the "stable" chart repository
  Update Complete. ⎈ Happy Helming!⎈ 
  
  $ kubectl create namespace monitoring
  namespace/monitoring created
  ```

- 安装 kube-prometheus-stack。

  这将会在monitoring命名空间下安装Prometheus Operator，而且将会创建crd，AlertManager， Promethues， ServiceMonitor等。

  ```
   helm install mon \
      --namespace monitoring \
      prometheus-community/kube-prometheus-stack
  ```

   ```
   $ kubectl get crd --namespace monitoring
   NAME                                        CREATED AT
   alertmanagers.monitoring.coreos.com         2020-12-18T13:05:10Z
   podmonitors.monitoring.coreos.com           2020-12-18T13:05:10Z
   prometheuses.monitoring.coreos.com          2020-12-18T13:05:10Z
   servicemonitors.monitoring.coreos.com       2020-12-18T13:05:11Z
   …
   ```

-  检查所有组件是否都部署正确

  ```
  $ kubectl get pods --namespace monitoring
  NAME                                                    READY   STATUS    RESTARTS   AGE
  alertmanager-mon-kube-prometheus-stack-alertmanager-0   2/2     Running   0          7m1s
  mon-grafana-5f64b7d85c-z58lc                            2/2     Running   0          7m19s
  mon-kube-prometheus-stack-operator-7886b467cd-t5x8l     1/1     Running   1          7m19s
  mon-kube-state-metrics-84cc9dd77b-fj8nx                 1/1     Running   2          7m19s
  mon-prometheus-node-exporter-8w5q6                      1/1     Running   1          7m19s
  prometheus-mon-kube-prometheus-stack-prometheus-0       2/2     Running   1          41s
  ```

##### 部署 mockmetrics 应用程序

`/metrics`它是一个用 Go 编写的简单 Web 服务器，它公开了端点的总命中数。我们将为它创建一个部署和服务。

- 这将在命名空间中创建 Deployment、Service、`default`HorizontalPodAutoscaler，并在命名空间中创建 ServiceMonitor `monitoring`。

  ```
  $ kubectl apply -f deploy/metrics-app/
  deployment.apps/mockmetrics-deploy created
  horizontalpodautoscaler.autoscaling/mockmetrics-app-hpa created
  servicemonitor.monitoring.coreos.com/mockmetrics-sm created
  service/mockmetrics-service created
  
  $ kubectl get svc,hpa
  NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)              AGE
  service/mockmetrics-service   ClusterIP   10.111.240.149   <none>        80/TCP               51s
  
  NAME                                                      REFERENCE                       TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
  horizontalpodautoscaler.autoscaling/mockmetrics-app-hpa   Deployment/mockmetrics-deploy   <unknown>/100   1         10        1          51s
  ```

  *一旦我们部署了自定义指标 API 服务器，该`<unknown>`字段就会有一个值。*

  ServiceMonitor 将被 Prometheus 拉取数据，因此它告诉 Prometheus`/metrics`每 10 秒从 mockmetrics 应用程序中抓取指标。

  ```yaml
  # deploy/metrics-app/mockmetrics-service-monitor.yaml
  
  apiVersion: monitoring.coreos.com/v1
  kind: ServiceMonitor
  metadata:
    name: mockmetrics-sm
    labels:
      release: mon
  spec:
    jobLabel: mockmetrics
    selector:
      matchLabels:
        app: mockmetrics-app
    endpoints: # 监听svc下的/metrics，Prometheus并以10s每次的方式拉取
    - port: metrics-svc-port
      interval: 10s
      path: /metrics
  ```

  ```yaml
  # deploy/metrics-app/mockmetrics-service-monitor.yaml
  
  apiVersion: autoscaling/v2beta1
  kind: HorizontalPodAutoscaler
  metadata:
    name: mockmetrics-app-hpa
  spec:
    scaleTargetRef: # 对mockmetrics-deploy进行操作
      apiVersion: apps/v1
      kind: Deployment
      name: mockmetrics-deploy
    minReplicas: 1
    maxReplicas: 10 
    metrics:
    - type: Object # 自定义指标
      object:
        target:
          kind: Service
          name: mockmetrics-service
        metricName: total_hit_count
        targetValue: 100
  ```

-  检查`mockmetrics-service`Prometheus 仪表板中是否显示为目标。

  ```
  $ kubectl port-forward svc/mon-kube-prometheus-stack-prometheus 9090 --namespace monitoring
  Forwarding from 127.0.0.1:9090 -> 9090
  Forwarding from [::1]:9090 -> 9090
  ```

##### 部署自定义指标 API 服务器（Prometheus 适配器）

- [使用prometheus-adapter](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter)图表创建部署自定义指标 API 服务器所需的资源。

  ```
  $ kubectl create namespace prometheus-adapter
  namespace/prometheus-adapter created
  
  $ helm install prometheus-adapter \
      --namespace prometheus-adapter \
      -f deploy/custom-metrics-server/prometheus-adapter-values.yaml \
      prometheus-community/prometheus-adapter
  ```

- 检查一切是否按预期运行。

  ```
  $ kubectl get svc --namespace prometheus-adapter 
  NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
  prometheus-adapter   ClusterIP   10.109.185.238   <none>        443/TCP   2m22s
  
  $ kubectl get apiservice | grep -E '^NAME|v1beta1.custom.metrics.k8s.io'
  ```

- 通过查询 API 检查是否收集了指标`custom.metrics.k8s.io/v1beta1`。
  ```
  $ kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/services/*/total_hit_count" | jq
  ```

##### 扩展应用程序

- 检查`mockmetrics-app-hpa`.

  ```
  $ kubectl get hpa
  NAME                  REFERENCE                       TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   0/100     1         10        1          38m
  ```

- mockmetrics 应用程序具有以下端点。
  - `/scale/up`: 不断增加`total_hit_count`被`/metrics`访问的时间
  - `/scale/down`: 开始减少值
  - `/scale/stop`: 停止增加或减少值

- 打开一个新的终端选项卡。

  ```
  $ kubectl port-forward svc/mockmetrics-service 8080:80 &
  Forwarding from 127.0.0.1:8080 -> 8080
  Forwarding from [::1]:8080 -> 8080
  
  $ curl localhost:8080/scale/
  stop
  ```

  让我们设置应用程序来增加计数器。

  ```
  $ curl localhost:8080/scale/up
  Going up!
  ```

​		由于 Prometheus 配置为每 10 秒抓取一次指标，因此值`total_hit_count`将不断变化。

- 现在在不同的终端选项卡中，让我们看看 HPA。

  ```
  $ kubectl get hpa -w
  NAME                  REFERENCE                       TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   6/100     1         10        1          40m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   30/100    1         10        1          40m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   72/100    1         10        1          41m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   90/100    1         10        1          41m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   132/100   1         10        1          41m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   78/100    1         10        2          42m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   114/100   1         10        2          42m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   92/100    1         10        3          42m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   116/100   1         10        3          43m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   93/100    1         10        4          43m
  …
  ```

  一旦该值大于目标，HPA 将自动增加`mockmetrics-deploy`.

- 要降低值，请在第一个终端选项卡中执行以下命令。

  ```
  $ curl localhost:8080/scale/down
  Going down :P
  
  $ kubectl get hpa -w
  NAME                  REFERENCE                       TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
  …
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   0/100        1         10        2          52m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   0/100        1         10        2          52m
  mockmetrics-app-hpa   Deployment/mockmetrics-deploy   0/100        1         10        1          53m
  ```

  

